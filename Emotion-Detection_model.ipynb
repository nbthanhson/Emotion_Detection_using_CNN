{"nbformat":4,"nbformat_minor":0,"metadata":{"accelerator":"GPU","colab":{"name":"Emotion-Detection_model.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyOcPBgheQQfcwOEh1v8epCM"},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"6wHghNN7GMqQ"},"source":["from google.colab import drive\n","drive.mount('/content/gdrive')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"cTVPIk75odKZ"},"source":["import pandas as pd\n","import numpy as np\n","\n","#Load dataset\n","path = '/content/gdrive/MyDrive/Emotion Detection/Data/fer2013.csv'\n","df = pd.read_csv(path)\n","\n","#Split data into training and testing set\n","train_x, train_y, test_x, test_y = [], [], [], []\n","for index, row in df.iterrows():\n","    val = row['pixels'].split(' ')\n","    try:\n","        if 'Training' in row['Usage']:\n","            train_x.append(np.array(val, 'float32'))\n","            train_y.append(row['emotion'])\n","        elif 'PublicTest' in row['Usage']:\n","            test_x.append(np.array(val, 'float32'))\n","            test_y.append(row['emotion'])\n","    except:\n","        print('Error occured at index: {} and row: {}'.format(index, row))\n","\n","print('Training data (length: {}): '.format(len(train_x)))\n","print(train_x)\n","print('Training label (length: {}): '.format(len(train_y)))\n","print(train_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_cvf-TFSzlE5"},"source":["from tensorflow import keras\n","from keras.utils import np_utils\n","\n","train_x = np.array(train_x, 'float32')\n","train_y = np.array(train_y, 'float32')\n","test_x = np.array(test_x, 'float32')\n","test_y = np.array(test_y, 'float32')\n","\n","train_y = np_utils.to_categorical(train_y, num_classes = 7)\n","test_y = np_utils.to_categorical(test_y, num_classes = 7)\n","\n","train_x /= 255.0\n","train_x -= 0.5\n","train_x *= 2.0\n","test_x /= 255.0\n","test_x -= 0.5\n","test_x *= 2.0\n","\n","train_x = train_x.reshape(train_x.shape[0], 48, 48, 1)\n","test_x = test_x.reshape(test_x.shape[0], 48, 48, 1)\n","\n","print('Training data (length: {}): '.format(len(train_x)))\n","print(train_x)\n","print('Training label (length: {}): '.format(len(train_y)))\n","print(train_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QhQb4QcXzIeG","executionInfo":{"status":"ok","timestamp":1619713402392,"user_tz":-420,"elapsed":85889,"user":{"displayName":"Nguyễn Thanh Sơn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGH7gZdp6Zo4_FVy6zO5DwGEdPwFirM_sARsdYZw=s64","userId":"09639000402480721945"}}},"source":["#Split training set into training and validation data\n","length_samples = len(train_x)\n","length_train_samples = int((1 - 0.2) * length_samples)\n","\n","train_data = train_x[:length_train_samples]\n","train_label = train_y[:length_train_samples]\n","\n","val_x = train_x[length_train_samples:]\n","val_y = train_y[length_train_samples:]\n","val_data = (val_x, val_y)"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"uiQijp8i9eP-"},"source":["from keras.layers import Conv2D, MaxPool2D, Dense, Flatten, Dropout, BatchNormalization\n","from keras.models import Sequential\n","\n","model = Sequential()\n","\n","#Add the first Block\n","model.add(Conv2D(filters = 32, kernel_size = (3, 3), padding = 'same', activation = 'relu', input_shape = train_data.shape[1:]))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters = 32, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size = (2, 2)))\n","model.add(Dropout(0.2))\n","\n","#Add the second Block\n","model.add(Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters = 64, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size = (2, 2)))\n","model.add(Dropout(0.2))\n","\n","#Add the third Block\n","model.add(Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters = 128, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size = (2, 2)))\n","model.add(Dropout(0.2))\n","\n","#Add the four Block\n","model.add(Conv2D(filters = 256, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n","model.add(BatchNormalization())\n","model.add(Conv2D(filters = 256, kernel_size = (3, 3), padding = 'same', activation = 'relu'))\n","model.add(BatchNormalization())\n","model.add(MaxPool2D(pool_size = (2, 2)))\n","model.add(Dropout(0.2))\n","\n","#Add the Flatten layer\n","model.add(Flatten())\n","\n","#Add the ouput layer\n","model.add(Dense(units = 256, activation = 'relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(units = 64, activation = 'relu'))\n","model.add(Dropout(0.5))\n","model.add(Dense(units = 7, activation = 'softmax'))\n","\n","#Display model summary\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Lhr9V0jeteHU","executionInfo":{"status":"ok","timestamp":1619713406616,"user_tz":-420,"elapsed":90105,"user":{"displayName":"Nguyễn Thanh Sơn","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GjGH7gZdp6Zo4_FVy6zO5DwGEdPwFirM_sARsdYZw=s64","userId":"09639000402480721945"}}},"source":["from keras.preprocessing.image import ImageDataGenerator\n","\n","data_generator = ImageDataGenerator(\n","                        featurewise_center = False, \n","                        featurewise_std_normalization = False,\n","                        rotation_range = 10,\n","                        width_shift_range = 0.1,\n","                        height_shift_range = 0.1,\n","                        zoom_range = 0.1,\n","                        horizontal_flip = True)"],"execution_count":6,"outputs":[]},{"cell_type":"code","metadata":{"id":"TV28cuZ7CiMl"},"source":["from keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n","\n","#Compile the model\n","model.compile(optimizer = 'adam', loss = 'categorical_crossentropy', metrics = ['accuracy'])\n","\n","#Early stop training\n","early_stopping = EarlyStopping(monitor = 'val_loss', patience = 10, verbose = 1)\n","\n","#Save the best model\n","best_model = ModelCheckpoint(filepath = 'Emotion_Detection_bestmodel.h5', monitor = 'val_loss', verbose = 1, save_best_only = True)\n","\n","#Reduce learning rate\n","reduce_lr = ReduceLROnPlateau(monitor = 'val_loss', factor = 0.2, patience = 5, verbose = 1, min_lr = 0.0001)\n","\n","#Start training\n","model_history = model.fit(\n","                        data_generator.flow(train_data, train_label, batch_size = 64),\n","                        epochs = 50,\n","                        validation_data = val_data,\n","                        callbacks = [early_stopping, best_model, reduce_lr],\n","                        shuffle = True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BlivG6XaES1t"},"source":["#Load the best model\n","keras.models.load_model(filepath = 'Emotion_Detection_bestmodel.h5')\n","\n","model.evaluate(test_x, test_y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EQaNu6HuihJ4"},"source":["import matplotlib.pyplot as plt\n","\n","# Get training loss and validation loss from model history\n","history_dict = model_history.history\n","loss = history_dict['loss']\n","val_loss = history_dict['val_loss']\n","\n","# Diplay a chart of training loss and validation loss\n","epochs = range(1, len(loss) + 1)\n","plt.plot(epochs, loss)\n","plt.plot(epochs, val_loss)\n","\n","plt.title('Training and validation loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend(['Training loss', 'Val loss'], loc='center right')\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lvnSol_merlf"},"source":["# Get training accuracy and validation loss from model history\n","history_dict = model_history.history\n","accuracy = history_dict['accuracy']\n","val_accuracy = history_dict['val_accuracy']\n","\n","# Diplay a chart of training accuracy and validation accuracy\n","epochs = range(1, len(accuracy) + 1)\n","plt.plot(epochs, accuracy)\n","plt.plot(epochs, val_accuracy)\n","\n","plt.title('Training and validation accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend(['Training accuracy', 'Val accuracy'], loc='center right')\n","\n","plt.show()"],"execution_count":null,"outputs":[]}]}